{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bby-mCQt-9XM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWWGwySCBvEg",
        "outputId": "4ed8daf3-ba91-440e-840f-312e5a962dd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.7-py3-none-any.whl (815 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/815.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/815.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.9/815.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.20 (from langchain)\n",
            "  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.22 (from langchain)\n",
            "  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.91-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.7 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 #helps read from pdf files\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken #creates token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3NSAbYIaPtZ",
        "outputId": "751b18f3-2d9d-4dd9-be1a-221be07ac180"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m194.6/232.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "03nBo08_aXWz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfreader = PdfReader('Writing Sample - Paper.pdf')"
      ],
      "metadata": {
        "id": "_XRhkm8GcI7n"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading text from pdf\n",
        "\n",
        "from typing_extensions import Concatenate\n",
        "\n",
        "raw_content = \"\"\n",
        "\n",
        "for i , page in enumerate(pdfreader.pages):\n",
        "  content = page.extract_text()\n",
        "  if content:\n",
        "    raw_content += content"
      ],
      "metadata": {
        "id": "40Qv3Y1OcekF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rfJ3AIsIc6WT",
        "outputId": "d12d8224-9e68-4f39-85f3-914b63dafb6a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'De-SaTE: Denoising Self-attention Transformer\\nEncoders for Li-ion Battery Health Prognostics\\nGaurav Shinde∗, Rohan Mohapatra†, Pooja Krishan†, and Saptarshi Sengupta†\\n∗Department of Engineering, San Jos ´e State University, San Jose, CA, USA\\n†Department of Computer Science, San Jos ´e State University, San Jose, CA, USA\\nEmail:∗gauravyeshwant.shinde@sjsu.edu,†rohan.mohapatra@sjsu.edu,\\n†pooja.krishan@sjsu.edu,†saptarshi.sengupta@sjsu.edu\\nAbstract —The usage of Lithium-ion (Li-ion) batteries has\\ngained widespread popularity across various industries, from\\npowering portable electronic devices to propelling electric vehi-\\ncles and supporting energy storage systems. A central challenge\\nin Li-ion battery reliability lies in accurately predicting their\\nRemaining Useful Life (RUL), which is a critical measure\\nfor proactive maintenance and predictive analytics. This study\\npresents a novel approach that harnesses the power of multiple\\ndenoising modules, each trained to address specific types of noise\\ncommonly encountered in battery data. Specifically, a denoising\\nauto-encoder and a wavelet denoiser are used to generate\\nencoded/decomposed representations, which are subsequently\\nprocessed through dedicated self-attention transformer encoders.\\nAfter extensive experimentation on NASA and CALCE data, a\\nbroad spectrum of health indicator values are estimated under a\\nset of diverse noise patterns. The reported error metrics on these\\ndata are on par with or better than the state-of-the-art reported\\nin recent literature.\\nIndex Terms —Prognostics and Health Management, Remain-\\ning Useful Life, Denoising Auto-Encoders, Lithium-ion Batteries,\\nTransformer, Battery Health\\nI. I NTRODUCTION\\nLithium-ion batteries (Li-ion) are the leading energy storage\\nsolution, prized for their exceptional energy density, rapid\\npower response, recyclability, and portability. Their unparal-\\nleled combination of energy and power density has made them\\nthe preferred choice for applications ranging from hybrid and\\nelectric vehicles to portable electronics. But, Li-ion battery\\ncapacity can deteriorate with time, influenced by factors such\\nas temperature, state of charge, cycling rate, and operating con-\\nditions, leading to reduced performance and potential failure.\\nThe battery capacity is a key health indicator, crucial for ac-\\ncurately forecasting the Remaining Useful Life (RUL) before\\nreaching End of Life (EOL) when performance significantly\\ndegrades or rated capacity can no longer be sustained.\\nPrognostics and Health Management (PHM) encompasses\\ndata acquisition, diagnostics, and the core component of\\nprognostics, which predicts a system’s RUL. In the context of\\nLi-ion batteries, PHM provides critical insights into monitor-\\ning their health, guiding maintenance decisions, and reducing\\nthe risk of unexpected failures, particularly in safety-critical\\napplications like electric vehicles and aerospace systems.\\nIn the past, RUL prediction primarily relied on conven-\\ntional machine learning models such as Convolutional Neural\\nNetwork (CNN) [1], Recurrent Neural network (RNN) [2],and Long Short Term Memory (LSTM) [3] networks. These\\nmodels, while valuable, often faced challenges in capturing\\nlong-term dependencies in sequential data and required manual\\nfeature engineering. Consequently, they are less flexible in\\nhandling diverse datasets and complex real-world scenarios.\\nRecent RUL prediction trends focus on attention-based\\nmechanisms, known for autonomously capturing intricate tem-\\nporal and spatial data dependencies, reducing the requirement\\nfor extensive feature engineering. This shift away from tra-\\nditional approaches aims to boost prediction accuracy and\\nadaptability in complex systems [4]. Chen et. al [5] explore the\\nutilization of a Denoising Auto-Encoder to enhance data rep-\\nresentation from battery inputs, which inherently exhibit noise\\ndue to various factors. However, it’s essential to acknowledge\\nthat measurement noise in practical scenarios may not adhere\\nto only a Gaussian distribution as assumed here. To address\\nthis concern and ensure robust noise handling, this research\\nproposes a novel approach. Diverse set of noise types are\\nimplemented in the denoising framework, where each noise\\ntype is associated with a dedicated auto-encoder and its cor-\\nresponding transformer encoder. The decoded representations\\nfrom these auto-encoders undergo transformation through the\\nrespective transformer encoders. Then, a minimization layer\\nwhich identifies the noise type that yields the minimum error\\nvalue is introduced. This adaptive noise modeling approach\\nbolsters the auto-encoder’s capacity to capture the spectrum\\nof non-Gaussian noise characteristics commonly encountered\\nin battery data. Due to the denoising process, this method\\nculminates in generating higher-quality data representations\\nfor subsequent analysis, where the prediction is based on the\\nnoise type associated with the minimum error.\\nContributions: The major contributions of this paper are:\\n1)Multi-Faceted Noise Mitigation: This work introduces\\na comprehensive noise mitigation strategy by employing\\ndedicated denoising auto-encoders and wavelet decom-\\nposers for various noise types present in battery opera-\\ntional data. Each denoising module is tailored to handle\\nspecific noise characteristics, enhancing the model’s\\nadaptability and relevance in real-world scenarios.\\n2)Robustness to different magnitudes of noise: The\\nproposed denoisers are tuned to handle various noise\\nlevels and noise distributions, thus making the architec-\\nture more robust to fluctuations in the input.arXiv:2310.00023v2  [cs.LG]  11 Nov 2023Outputs\\nHidden Layer 3\\nHidden Layer 2\\nHidden Layer 1\\n+Uniform\\nNoiseMulti-Head\\nTime AttentionAdd & NormFeed\\nForwardAdd & Norm\\nPositional \\nEncoding\\nOutputs\\nHidden Layer 3\\nHidden Layer 2\\nHidden Layer 1\\n+Gaussian\\nNoiseOutputs\\nHidden Layer 3\\nHidden Layer 2\\nHidden Layer 1\\n+Speckle\\nNoiseOutputs\\nHidden Layer 3\\nHidden Layer 2\\nHidden Layer 1\\n+Poisson\\nNoiseLinear\\nMulti-Head\\nTime AttentionAdd & NormFeed\\nForwardAdd & Norm\\nPositional \\nEncodingLinear\\nMulti-Head\\nTime AttentionAdd & NormFeed\\nForwardAdd & Norm\\nPositional \\nEncodingLinear\\nMulti-Head\\nTime AttentionAdd & NormFeed\\nForwardAdd & Norm\\nPositional \\nEncodingLinearMin LayerOutputs\\nInputsNormalizationSelf Attention\\nNetwork\\nDenoiserOutputs\\nIDWT\\nThresholding via\\nDifferent Modes\\nWavelet\\nDecompositionMulti-Head\\nTime AttentionAdd & NormFeed\\nForwardAdd & Norm\\nPositional \\nEncodingLinearFig. 1: The proposed De-SaTE : Denoising Self-attention\\nTransformer Encoder architecture\\n3)Better data representation leading to enhanced ac-\\ncuracy: This architecture effectively denoises the input\\ndata and improves the quality of data representations\\nleading to better predictions.\\n4)A modular architecture for all complex processing:\\nThe proposed architecture processes the input by passing\\nit through the denoising modules to encode various types\\nof noisy data, and the self-attention encoder network\\nsubsequently learns the degradation physics and predicts\\nthe remaining useful life.\\nII. S ELF-ATTENTION WITH VARIABLE DENOISING\\nIn the input data x={x1, x2, . . . , x n}, where x∈(0,1].\\nA normalized input is produced, x′=x\\nC0where C0denotes\\nrated capacity. Subsequently, multiple denoising schemes, each\\ntrained to mitigate a specific noise type are leveraged. The\\nencoded representations from these denoising modules are\\nthen subjected to individual self-attention layers. The data\\npasses through the self-attention layers, each one intricately\\nconnected to its respective noise reducer, which then yields\\nindividual metric values such as Relative Error (RE), Mean\\nAbsolute Error (MAE), and Root Mean Square Error (RMSE).\\nA minimization strategy is employed, which selects the min-\\nimum value among these metrics, thereby obtaining the most\\noptimal error estimate. This architecture ensures robust perfor-\\nmance and is highly effective in estimating failures, even when\\nconfronted with the presence of diverse noise types inherent\\nin battery data. The system architecture of the entire process\\nis shown in Fig. 1.\\nA. Positional Encoding\\nPE (pos, 2k)= sin\\x10pos\\n100002k/d k\\x11\\n(1)\\nPE (pos, 2k+1)= cos\\x10pos\\n100002k/d k\\x11\\n(2)\\nInput DataMatrix\\nMultiplicationScaleMask\\n(Optional)Soft Max\\nMatrix\\nMultiplicationQ\\nK\\nV(a) Scaled Dot-Product Self-Attention mechanism\\nScaled Dot-Product\\nSelf-AttentionScaled Dot-Product\\nSelf-AttentionQ\\nK\\nVLinear\\nLinear\\nLinearScaled Dot-Product\\nSelf-AttentionLinear Concat\\n(b) Multi-Head Self-Attention mechanism\\nFig. 2: Multi-Head Self-Attention architecture\\nB. Self Attention\\nThe encoder’s self-attention mechanism [6] computes atten-\\ntion scores for each position in the input sequence, allowing\\nthe model to weigh the importance of different elements in the\\nsequence when encoding a particular position. This is typically\\ncomputed using a weighted sum of queries, keys and values\\nas shown Fig. 2a.\\nAttention (Q, K, V ) =softmax (QK′\\n√dk)V (3)\\nHere, Qrepresents the query matrix for the input sequence,\\nKrepresents the key matrix for the input sequence, Vrep-\\nresents the value matrix for the input sequence and dkis a\\nscaling factor to stabilize the gradients.\\nC. Multi-head Attention\\nIn Fig. 2b, the multi-head attention process is outlined. The\\nself-attention mechanism is often used in multiple heads [6]\\nto capture different types of dependencies:\\nMultiHead (Q, K, V ) =Concat (head 1,head 2, ...,head h)·WO\\n(4)\\nHere, head irepresents the output of the i-th attention head\\nandWOis a learnable weight matrix.\\nD. Feed forward network\\nAfter computing attentions, the output passes through a\\nFeed-Forward Neural network:\\nFFN(x) =ReLU (W1·x+b1)·W2+b2 (5)\\nHere, W1,b1,W2,b2are learnable weights and biases.\\nE. Learning\\nThe proposed architecture can be divided into two tasks:\\ndenoising andmetric evaluation . The learning procedure opti-\\nmizes both tasks simultaneously within a unified framework.\\nMean Square Error (MSE) is used to evaluate loss, and the\\nobjective function L[5] is defined as follows:.\\nL=nX\\nt=T+1(xt−ˆxt)2+δnX\\ni=1ℓ(xcorr−ˆxi) +αψ(Lrate)(6)\\nwhere, nis the number of samples, δcontrols relative\\ncontribution of each task, ℓ(·)is the loss function, αis aregularization parameter, ψ(·)denotes regularization and Lrate\\ndenotes learning parameters.\\nThe denoising effect along with penalized loss acts like a\\nregularizer. Regularization techniques [7] add a penalty term to\\nthe loss function, boosting model performance to tend towards\\nsmaller weights or simpler representations.\\nIII. E XPERIMENTAL SETUP\\nA. Dataset Description\\nTwo datasets from National Aeronautics and Space Ad-\\nministration (NASA) and Center for Advanced Life Cycle\\nEngineering (CALCE) were used to conduct the experiments.\\nThe NASA dataset, acquired from the NASA Ames Research\\nCenter, comprises of records from four different Li-ion bat-\\nteries (B0005, B0006, B0007 and B0018), each subjected to\\nthree distinct operations: charging, discharging, and impedance\\nmeasurements [8], [9]. The CALCE dataset (CS2 35, CS2 36,\\nCS2 37 and CS2 38) is sourced from the Center for Advanced\\nLife Cycle Engineering (CALCE) at the University of Mary-\\nland [10]. Figures 3a and 3b illustrate the capacity degradation\\ntrends observed across various batteries in these datasets.\\n(a) Degradation trend on the\\nNASA dataset\\n(b) Degradation trend on the\\nCALCE dataset\\nFig. 3: Capacity vs. degradation cycles\\nB. Noise distributions\\n1) Gaussian Noise\\nGaussian noise, characterized by mean ( µ) and standard\\ndeviation ( σ) has the Probability Density Function (PDF):\\nf(x;µ, σ) =1\\nσ√\\n2πe−(x−µ)2\\n2σ2 (7)\\n2) Speckle Noise\\nSpeckle noise is often multiplicative, where input values are\\nmultiplied by random values. Its PDF is:\\nf(x;γ) =1\\nγ2e−x\\nγ (8)\\nwhere γis a parameter controlling the noise intensity.\\n3) Poisson Noise\\nPoisson noise is characterized by its mean ( λ). Poisson\\nnoise manifests when events occur at a consistent average\\nrate but with randomness in the exact timing or occurrence of\\nthese events. This noise, explained as instrumentation noise in\\nbattery health prognosis has the PDF:\\nf(x;λ) =e−λλx\\nx!(9)4) Uniform Noise\\nUniform noise characterized by a minimum value ( a) and\\na maximum value ( b), has the PDF:\\nf(x;a, b) =(\\n1\\nb−aifa≤x≤b\\n0 otherwise(10)\\nC. Denoising Autoencoder\\nA denoising autoencoder is effective in removing noise\\nfrom data and learning robust representations. The network\\nis trained to reconstruct clean data from noisy input. It\\nconsists of an encoder that maps the input data to a latent\\nrepresentation and a decoder that reconstructs the data from\\nthis representation. During training, the encoder learns to\\ncapture essential features while the decoder learns to remove\\nnoise. The loss function defined in Eqn. 6 typically measures\\nreconstruction error, encouraging the network to minimize\\ndifferences between clean inputs and reconstructed outputs.\\nLetx0t=x0(t+1), x0(t+2), . . . , x 0(t+m)∈x0denote the\\nslice of input with msamples of a sequence. A noise is added\\nto the normalized input to obtain the corrupted vector, xcorr.\\nDAE serves two purposes: denoising the raw input and\\nlearning a nonlinear representation [5]:\\nz=a(W·xcorr+b) (11)\\nwhere W,b,a(·), and zdenote weight, bias, activation\\nfunction, and the output of the DAE encoder.\\nThen, to reconstruct the input vector, the latent representa-\\ntion is mapped back to the input space, defined as follows:\\nˆxt=f0(W0·z+b0) (12)\\nwhere W0,b0,z, and f0(·)denote weight, bias, output, and\\nmap function of the output layer of the DAE encoder.\\nIn this network, identity and ReLU functions are used as\\nthe decoding and encoding activation, respectively. Finally, the\\nobjective function Ldis defined as follows:\\nLd=1\\nnnX\\nt=1(xcorr−ˆxt)2+α\\x00\\n∥W∥2\\nF+∥W0∥2\\nF\\x01\\n(13)\\nwhere ∥ · ∥ Fis the Fibonacci-norm, αis the regularization\\nparameter, and nis the number of samples.\\nD. Wavelet Transformation and Denoiser\\nWavelet denoising is a signal processing technique for\\nremoving noise from a voltage signal coming from a battery\\nor another source. Wavelet denoising [11] typically involves\\nthresholding coefficients obtained from wavelet transforms.\\nA typical transformation involves passing the signal through\\nDiscrete Wavelet Transform (DWT), thresholding the wavelet\\ncoefficients. The denoised signal is reconstructed using the\\ninverse DWT. The process is outlined below.\\n1) DWT\\nThe DWT decomposes a signal or image into wavelet\\ncoefficients at different scales and positions:\\nθ=DWT (I) (14)\\nIis the original signal, θcontains wavelet coefficients.2) Thresholding\\nThresholding is applied to the wavelet coefficients to remove\\nor reduce noise. A common method is Soft thresholding:\\nˆθi,j=sign(θi,j)·max (|θi,j| −ϵ,0) (15)\\nAnother approach to thresholding is Hard thresholding.\\nHard thresholding sets coefficients below a certain threshold\\nto zero and retains those above the threshold. It is defined as\\nfollows:\\nˆθi,j=(\\nθi,j,if|θi,j| ≥ϵ\\n0, if|θi,j|< ϵ(16)\\nAn additional thresholding method, Garrote , is a variation\\npenalizing large coefficients than smaller ones and is given as:\\nˆθi,j=sign(θi,j)·max (|θi,j| −ϵ,0)\\n1 +ϵ\\n|θi,j|(17)\\nwhere, ˆθi,jis the denoised coefficient, θi,jis the original\\ncoefficient, and ϵis the threshold value.\\n3) Inverse Discrete Wavelet Transform (IDWT)\\nIDWT reconstructs signals using this transformation:\\nˆI=IDWT (ˆθ) (18)\\nˆIis the denoised signal, ˆθcontains the denoised wavelet\\ncoefficients.\\nE. Training and Evaluation\\nFour types of noise were introduced: Gaussian, Speckle,\\nPoisson, and Uniform with varying noise levels (small,\\nmedium, and relatively high). The optimal hyperparameters\\nare evaluated by a grid search.\\n•Learning Rate: 1e-3 and 1e-2\\n•Number of Layers: 1 and 2\\n•Hidden Dimension: 16 and 32\\n•Noise Levels: 0.001, 0.01, and 0.05\\n•Epochs: 2000\\nThe experiments were run on a system using Python 3.10,\\nTensorflow 2.0 and Keras with Nvidia A100 and Nvidia T4\\nGPUs.\\nThe models are evaluated using three key metrics outlined\\nin Appendix B. This systematic exploration is aimed to iden-\\ntify the best hyperparameters for accurate predictions amidst\\ndiverse noise distributions, ensuring robustness and scalability.\\nIV. R ESULTS\\nFigures 4a and 4b demonstrate the RE, MAE, and RMSE\\nvalues under various noise distributions for the NASA and\\nCALCE datasets. A comparative analysis as shown in Fig. 5\\nof how each noise affects the key metrics used in this paper\\nis performed. The results are tabulated in Tables I and II.\\nThe network performed optimally on the NASA dataset\\nwith a Learning rate (LR) of 0.01, Number of layers (NoL)\\nof 1, 16 Hidden Dimensions (HD), and an αof 1e-05.\\nIn contrast, the optimal parameters for the CALCE dataset\\ninvolve a LR of 0.001, a NoL of 1, 32 HD, and an αof 0.01.TABLE I: Results for the NASA dataset\\nNoise and metrics LR NoL HD α NL Result\\nGaussianRE 0.01 1 16 1e-05 0.05 0.1674\\nMAE 0.01 1 16 1e-05 0.05 0.0806\\nRMSE 0.01 2 16 1e-05 0.01 0.0957\\nSpeckleRE 0.01 1 16 0.0001 0.05 0.1869\\nMAE 0.01 1 16 1e-05 0.01 0.0807\\nRMSE 0.01 1 16 1e-05 0.01 0.0935\\nPoissonRE 0.01 2 16 0.0001 0.01 0.1876\\nMAE 0.01 2 16 1e-5 0.05 0.0860\\nRMSE 0.01 2 32 1e-5 0.001 0.1013\\nUniformRE 0.001 2 32 1e-5 0.05 0.2285\\nMAE 0.001 1 32 0.0001 0.001 0.0891\\nRMSE 0.001 1 32 0.0001 0.001 0.0781\\nTABLE II: Results for the CALCE dataset\\nNoise and metrics LR NoL HD α NL Result\\nGaussianRE 0.001 1 32 0.01 0.001 0.052\\nMAE 0.001 1 32 0.01 0.01 0.008\\nRMSE 0.001 1 32 0.01 0.01 0.09\\nSpeckleRE 0.001 1 32 0.01 0.001 0.052\\nMAE 0.001 1 32 0.01 0.01 0.008\\nRMSE 0.001 1 32 0.01 0.01 0.091\\nPoissonRE 0.001 1 32 0.01 0.01 0.033\\nMAE 0.001 1 32 0.01 0.01 0.024\\nRMSE 0.001 1 32 0.01 0.01 0.152\\nUniformRE 0.001 1 32 0.01 0.001 0.052\\nMAE 0.001 1 32 0.01 0.001 0.009\\nRMSE 0.001 1 32 0.01 0.001 0.093\\nThese configurations were fine-tuned for the auto-encoder and\\ntransformer encoder layers via a grid search.\\nRE is highly related to the RUL of a battery, and serves\\nas the primary evaluation metric. After thorough experimen-\\ntation, it is observed that RE achieved superior results when\\npaired with a denoising autoencoder followed by a transformer\\nencoder. The three wavelet denoising modes - Soft, Hard, and\\nGarrote - each with three distinct thresholds (0.001, 0.01, and\\n0.05) are explored to comprehensively assess their impact on\\nthe overall performance. Results are tabulated in Table III.\\nTABLE III: Mean RE distribution for different wavelet modes\\nand thresholds\\nDataset Wavelet Denoising Mode Threshold\\n0.001 0.01 0.05\\nNASASoft 0.29 0.31 0.52\\nHard 0.213 0.12 0.17\\nGarotte 0.24 0.27 0.22\\nCALCESoft 0.76 0.78 0.81\\nHard 0.56 0.61 0.62\\nGarotte 0.65 0.67 0.71\\nV. C ONCLUSION AND FUTURE WORK\\nThis work uses a denoising framework to filter out noise\\nfrom the NASA and CALCE lithium-ion battery data to\\nestimate the RE, MAE, and RMSE metrics. In addition to\\nusual modeling of Gaussian noise, this study extends the recent\\nliterature to model multiple types of noise distributions. The\\nfindings show that Poisson noise produces a lower RE of 0.033(a) Mean Relative Error (RE), Root Mean Squared Error (RMSE),\\nMean Absolute Error (MAE) values on the NASA data under different\\ntypes of noise with varying levels\\n(b) Mean RE, RMSE, MAE values on the CALCE data under different\\ntypes of noise with varying levels\\nFig. 4: Comparison of metrics under diverse types and levels of noise\\n(a) Poisson noise with lower RE\\nand RMSE but higher MAE than\\nUniform noise on NASA Dataset\\n(evaluated with Denoising Au-\\ntoencoder)\\n(b) Poisson noise with lower\\nRE but higher RMSE and MAE\\nthan Gaussian noise on CALCE\\nDataset (evaluated with Denois-\\ning Autoencoder)\\nFig. 5: Effects of different noise on metrics (RE, RMSE, MAE)\\nTABLE IV: Model Evaluation on Li-ion Battery Datasets\\nDataset Model Metrics\\nRE MAE RMSE\\nNASAMLP [12] 0.3851 0.1379 0.1541\\nRNN [2] 0.2851 0.0749 0.0848\\nLSTM [3] 0.2648 0.0829 0.0905\\nGRU [13] 0.3044 0.0806 0.0921\\nDual-LSTM [14] 0.2557 0.0815 0.0879\\nDeTransformer [5] 0.2252 0.0713 0.0802\\nDe-SaTE 0.1674 0.0806 0.0781\\nCALCEMLP [12] 0.4018 0.1557 0.2038\\nRNN [2] 0.1614 0.0938 0.1099\\nLSTM [3] 0.0902 0.0582 0.0736\\nGRU [13] 0.1319 0.0671 0.0946\\nDual-LSTM [14] 0.0885 0.0636 0.0874\\nDeTransformer [5] 0.0764 0.0613 0.0705\\nDe-SaTE 0.0330 0.0080 0.090over the other noises for the CALCE dataset. However, Gaus-\\nsian noise yields enhanced performance across RE and MAE\\nfor the NASA dataset. The proposed architecture produces\\nlower RE, MAE, and RMSE compared to past work.\\nIn future, the response of the proposed architecture to\\nadversarial attacks may be proposed and defense strategies\\nmay be devised accordingly, thereby adding to its robustness.\\nAs a next step, a more traditional encoder-decoder model\\nmight be introduced to extend the present capabilities and\\nadvance the understanding of battery health prognostics.\\nACKNOWLEDGMENT\\nThe research reported in this publication was supported\\nby the Division of Research and Innovation at San Jos ´e\\nState University under Award Number 23-UGA-08-044. The\\ncontent is solely the responsibility of the author(s) and does\\nnot necessarily represent the official views of San Jos ´e State\\nUniversity.\\nREFERENCES\\n[1] D. Gao, X. Liu, Z. Zhu, and Q. Yang, “A hybrid cnn-bilstm approach for\\nremaining useful life prediction of evs lithium-ion battery,” Measurement\\nand Control , vol. 56, no. 1-2, pp. 371–383, 2023.\\n[2] J. Liu, A. Saxena, K. Goebel, B. Saha, and W. Wang, “An adaptive\\nrecurrent neural network for remaining useful life prediction of\\nlithium-ion batteries,” Annual Conference of the PHM Society , vol. 2,\\nno. 1, 2010. [Online]. Available: https://papers.phmsociety.org/index.\\nphp/phmconf/article/view/1896\\n[3] Y . Zhang, R. Xiong, H. He, and M. G. Pecht, “Long short-\\nterm memory recurrent neural network for remaining useful life\\nprediction of lithium-ion batteries,” IEEE Transactions on Vehicular\\nTechnology , vol. 67, no. 7, pp. 5695–5705, 2018. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/8289406\\n[4] P. Khumprom and N. Yodo, “A data-driven predictive prognostic\\nmodel for lithium-ion batteries based on a deep learning algorithm,”\\nEnergies , vol. 12, no. 4, 2019. [Online]. Available: https://www.mdpi.\\ncom/1996-1073/12/4/660\\n[5] D. Chen, W. Hong, and X. Zhou, “Transformer network for\\nremaining useful life prediction of lithium-ion batteries,” IEEE\\nAccess , vol. 10, pp. 19 621–19 628, 2022. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/9714323[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you\\nneed,” in Advances in Neural Information Processing Systems ,\\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\\nS. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,\\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper\\nfiles/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\n[7] R. Kress, Tikhonov Regularization . Berlin, Heidelberg: Springer\\nBerlin Heidelberg, 1989, pp. 243–258. [Online]. Available: https:\\n//doi.org/10.1007/978-3-642-97146-4 16\\n[8] C. Kulkarni and A. Guarneros, “Small satellite power simulation\\ndata set,” NASA Prognostics Data Repository, NASA Ames Research\\nCenter, Moffett Field, CA, 2008. [Online]. Available: https://www.nasa.\\ngov/content/prognostics-center-of-excellence-data-set-repository\\n[9] B. Saha and K. Goebel, “Uncertainty management for diagnostics\\nand prognostics of batteries using bayesian techniques,” 04\\n2008, pp. 1 – 8. [Online]. Available: https://www.researchgate.\\nnet/publication/224314763 Uncertainty Management forDiagnostics\\nand Prognostics ofBatteries using Bayesian Techniques\\n[10] Y . Xing, E. W. Ma, K.-L. Tsui, and M. Pecht, “An ensemble model for\\npredicting the remaining useful performance of lithium-ion batteries,”\\nMicroelectronics Reliability , vol. 53, no. 6, pp. 811–820, 2013.\\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\\nS0026271412005227\\n[11] D. Cho and T. D. Bui, “Multivariate statistical modeling for\\nimage denoising using wavelet transforms,” Signal Processing: Image\\nCommunication , vol. 20, no. 1, pp. 77–89, 2005. [Online]. Available:\\nhttps://www.sciencedirect.com/science/article/pii/S0923596504000979\\n[12] Y . Wu, W. Li, Y . Wang, and K. Zhang, “Remaining useful life\\nprediction of lithium-ion batteries using neural network and bat-based\\nparticle filter,” IEEE Access , vol. 7, pp. 54 843–54 854, 2019. [Online].\\nAvailable: https://ieeexplore.ieee.org/document/8698866\\n[13] B. Xiao, Y . Liu, and B. Xiao, “Accurate state-of-charge estimation\\napproach for lithium-ion batteries by gated recurrent unit with\\nensemble optimizer,” IEEE Access , vol. PP, pp. 1–1, 04 2019. [Online].\\nAvailable: https://www.researchgate.net/publication/332650682\\nAccurate State-of-Charge Estimation Approach forLithium-Ion\\nBatteries byGated Recurrent Unit With Ensemble Optimizer\\n[14] Z. Shi and A. Chehade, “A dual-lstm framework combining change point\\ndetection and remaining useful life prediction,” Reliability Engineering\\n& System Safety , vol. 205, p. 107257, 2021. [Online]. Available:\\nhttps://www.sciencedirect.com/science/article/pii/S0951832020307572\\n[15] Z. Huang, Z. Xu, W. Wang, and Y . Sun, “Remaining useful life\\nprediction for a nonlinear heterogeneous wiener process model with\\nan adaptive drift,” IEEE Transactions on Reliability , vol. 64, no. 2,\\npp. 687–700, 2015. [Online]. Available: https://ieeexplore.ieee.org/\\ndocument/7051292\\n[16] H. Hanachi, J. Liu, A. Banerjee, Y . Chen, and A. Koul, “A physics-\\nbased modeling approach for performance monitoring in gas turbine\\nengines,” IEEE Transactions on Reliability , vol. 64, no. 1, pp. 197–205,\\n2015. [Online]. Available: https://ieeexplore.ieee.org/document/6963507\\n[17] Y . Lei, N. Li, S. Gontarz, J. Lin, S. Radkowski, and\\nJ. Dybala, “A model-based method for remaining useful life\\nprediction of machinery,” IEEE Transactions on Reliability ,\\nvol. 65, no. 3, pp. 1314–1326, 2016. [Online]. Available:\\nhttps://www.researchgate.net/publication/304611910 AModel-Based\\nMethod forRemaining Useful Life Prediction ofMachinery\\n[18] A. Barr ´e, B. Deguilhem, S. Grolleau, M. G ´erard, F. Suard, and\\nD. Riu, “A review on lithium-ion battery ageing mechanisms\\nand estimations for automotive applications,” Journal of Power\\nSources , vol. 241, pp. 680–689, 2013. [Online]. Available: https:\\n//www.sciencedirect.com/science/article/pii/S0378775313008185\\n[19] “A comprehensive review of battery modeling and state estimation\\napproaches for advanced battery management systems,” Renewable\\nand Sustainable Energy Reviews , vol. 131, p. 110015, 2020.\\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\\nS1364032120303063\\n[20] M. Ahwiadi and W. Wang, “An enhanced mutated particle filter\\ntechnique for system state estimation and battery life prediction,”\\nIEEE Transactions on Instrumentation and Measurement , vol. 68,\\nno. 3, pp. 923–935, 2019. [Online]. Available: https://ieeexplore.ieee.\\norg/document/8421633\\n[21] L. Zhang, Z. Mu, and C. Sun, “Remaining useful life prediction for\\nlithium-ion batteries based on exponential model and particle filter,”IEEE Access , vol. 6, pp. 17 729–17 740, 2018. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/8318570\\n[22] “Battery health management for small-size rotary-wing electric\\nunmanned aerial vehicles: An efficient approach for constrained\\ncomputing platforms,” Reliability Engineering & System Safety , vol.\\n182, pp. 166–178, 2019. [Online]. Available: https://www.sciencedirect.\\ncom/science/article/pii/S0951832018301406\\n[23] J. Zhao, L. Tian, L. Cheng, Y . Zhang, and C. Zhu, “Review on rul\\nprediction methods for lithium-ion battery,” in 2022 IEEE/IAS Industrial\\nand Commercial Power System Asia (I&CPS Asia) , 2022, pp. 1501–\\n1505. [Online]. Available: https://ieeexplore.ieee.org/document/9949753\\n[24] Y . Zhu, Y . Shang, B. Duan, X. Gu, S. Li, and G. Chen, “A\\ndata-driven method for lithium-ion batteries remaining useful life\\nprediction based on optimal hyperparameters,” in 2022 41st Chinese\\nControl Conference (CCC) , 2022, pp. 7388–7392. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/9902792\\n[25] J. Zhou, D. Liu, Y . Peng, and X. Peng, “An optimized relevance\\nvector machine with incremental learning strategy for lithium-ion\\nbattery remaining useful life estimation,” in 2013 IEEE International\\nInstrumentation and Measurement Technology Conference (I2MTC) ,\\n2013, pp. 561–565. [Online]. Available: https://ieeexplore.ieee.org/\\ndocument/6555479\\n[26] X. Qin, Q. Zhao, H. Zhao, W. Feng, and X. Guan, “Prognostics\\nof remaining useful life for lithium-ion batteries based on a\\nfeature vector selection and relevance vector machine approach,”\\nin2017 IEEE International Conference on Prognostics and Health\\nManagement (ICPHM) , 2017, pp. 1–6. [Online]. Available: https:\\n//ieeexplore.ieee.org/document/7998297\\n[27] Y . Chen, C. Zhang, N. Zhang, X. Guo, H. Wang, and Y . Chen,\\n“Cuckoo search based relevance vector machine with hybrid kernel\\nfor battery remaining useful life prediction,” in 2019 Prognostics and\\nSystem Health Management Conference (PHM-Qingdao) , 2019, pp.\\n1–6. [Online]. Available: https://ieeexplore.ieee.org/document/8942856\\n[28] LITHIUM-ION BATTERY REMAINING USEFUL LIFE ESTIMATION\\nBASED ON ENSEMBLE LEARNING WITH LS-SVM ALGORITHM ,\\n2017, pp. 217–232. [Online]. Available: https://ieeexplore.ieee.org/\\ndocument/7656791\\n[29] “A novel multistage support vector machine based approach for li ion\\nbattery remaining useful life estimation,” Applied Energy , vol. 159,\\npp. 285–297, 2015. [Online]. Available: https://www.sciencedirect.com/\\nscience/article/pii/S0306261915010557\\n[30] “A comprehensive review of battery modeling and state estimation\\napproaches for advanced battery management systems,” Renewable\\nand Sustainable Energy Reviews , vol. 131, p. 110015, 2020.\\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\\nS1364032120303063\\n[31] Y . Zhang, R. Xiong, H. He, and Z. Liu, “A lstm-rnn method for the\\nlithuim-ion battery remaining useful life prediction,” in 2017 Prognostics\\nand System Health Management Conference (PHM-Harbin) , 2017, pp.\\n1–4. [Online]. Available: https://ieeexplore.ieee.org/document/8079316\\n[32] L. Ren, L. Zhao, S. Hong, S. Zhao, H. Wang, and L. Zhang,\\n“Remaining useful life prediction for lithium-ion battery: A deep\\nlearning approach,” IEEE Access , vol. 6, pp. 50 587–50 598, 2018.\\n[Online]. Available: https://ieeexplore.ieee.org/document/8418374\\n[33] Y . Liu, G. Zhao, and X. Peng, “Deep learning prognostics for\\nlithium-ion battery based on ensembled long short-term memory\\nnetworks,” IEEE Access , vol. 7, pp. 155 130–155 142, 2019. [Online].\\nAvailable: https://ieeexplore.ieee.org/abstract/document/8815721\\n[34] “State-of-health estimation and remaining useful life prediction for\\nthe lithium-ion battery based on a variant long short term memory\\nneural network,” Journal of Power Sources , vol. 459, p. 228069, 2020.\\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\\nS0378775320303724\\n[35] S. Zhang, B. Zhai, X. Guo, K. Wang, N. Peng, and X. Zhang,\\n“Synchronous estimation of state of health and remaining useful\\nlifetime for lithium-ion battery using the incremental capacity and\\nartificial neural networks,” Journal of Energy Storage , vol. 26,\\np. 100951, 2019. [Online]. Available: https://www.sciencedirect.com/\\nscience/article/pii/S2352152X19307340\\n[36] D. Zhou, Z. Li, J. Zhu, H. Zhang, and L. Hou, “State of\\nhealth monitoring and remaining useful life prediction of lithium-\\nion batteries based on temporal convolutional network,” IEEE\\nAccess , vol. 8, pp. 53 307–53 320, 2020. [Online]. Available: https:\\n//ieeexplore.ieee.org/document/9037249APPENDIX A\\nRELATED WORK\\nLi-ion batteries are a ubiquitous power source in industries\\nlike electric cars, drone technology, and various other applica-\\ntions, offering efficient energy storage. Battery prognosis has\\nseen rising trends with the widespread use of Li-ion batteries\\nin the industry. Recent literature [4] has seen drift from model-\\nbased techniques to deep-learning based models.\\nA. Model Based Methods\\nModel-based methods attempt to set up mathematical or\\nphysical models to describe degradation processes, and update\\nmodel parameters using measured data [15]–[17]. However,\\nthese methods require accurate knowledge of the battery’s\\ninternal structure and operating conditions, which can be\\nchallenging to obtain in practice.\\n1) Physical Model\\nPhysical model quantifies the factors that influence a bat-\\ntery’s performance. This approach usually focuses on the\\nspecific physical and chemical phenomena occurring during\\nutilization [18]. Physics-based models rely on mathematical\\nequations to describe the battery’s physical attributes and\\ncontrolling principles.\\n2) Electrochemical Model\\nElectrochemical models are based on precise mathemat-\\nical models of electrochemical processes that occur within\\nthe battery, such as chemical reactions, lithium ion and\\nelectron movement, and heat impacts [19]. However, due\\nto the complexity and non-linearity of battery behavior, as\\nwell as the challenge of precisely describing the electro-\\nchemical processes within the battery, establishing accurate\\nelectrochemical-based models can be difficult.\\n3) Adaptive Filter Method\\nAdaptive filter is a digital filter whose coefficient varies with\\nthe target for the filter to converge to the optimal state [20],\\n[21]. Multiple research studies have demonstrated that adaptive\\nfilters function well in RUL estimation of Li-ion batteries.\\n4) Stochastic Process Methods\\nStochastic process methods are based on the notion that\\nbattery degradation is a stochastic process that can be mod-\\neled using probabilistic methods. The advantage of stochastic\\napproaches is that they can represent the unpredictability and\\nuncertainty inherent in the battery deterioration process. But\\nthey may need more complicated modeling and computational\\ntechniques [22].\\nB. Data Driven Methods\\nData-driven methods rely extensively on analyzing the bat-\\ntery’s operating data to estimate its degradation level and\\npredict when it will reach the end of its useful life. Such\\na method can directly mime the degradation information of\\nlithium-ion battery through historical data, and there is no need\\nto establish a specific mathematical model [23], [24].1) Traditional Machine Learning\\nThis section provides an overview of traditional machine\\nlearning methods for RUL estimation of Li-ion batteries. The\\ntechniques that utilize Relevance Vector Machine (RVM) [25]–\\n[27] achieve efficient online training for updating the model\\nusing battery data. However, they pose a memory consumption\\nissue with increased model complexity. Multiple ensemble\\nmodels [27]–[30] have been proposed to improve prediction\\naccuracy. Over the years, traditional machine learning methods\\nhave been widely used for RUL estimation of Li-ion batteries.\\nWhile these methods have shown promising results, they also\\nhave their limitations. Overall, these methods offer a range\\nof approaches to address the challenge of RUL estimation and\\ncan be useful in various applications, but careful consideration\\nis needed when selecting the most appropriate method for a\\nparticular scenario.\\n2) The Advent of Deep Learning\\nDeep learning models automatically learn relevant features\\nfrom raw data, capture complex relationships between input\\nfeatures and output targets, and generalize well to new data.\\nLSTM models and their variants [31]–[34] can extract multi-\\ndimensional features and estimate RUL with high precision.\\nZhang et al. [35] proposed an online estimation method that\\ncombines partial incremental capacity with an Artificial Neural\\nNetwork (ANN) for estimating battery State of Health (SOH)\\nand RUL. Temporal Convolution Network (TCN) [36] that\\nuses causal and dilated convolution techniques to capture\\nlocal capacity regeneration has improved prediction accuracy.\\nOverall, the shift from traditional machine learning to deep\\nlearning has led to significant improvements in the accuracy\\nand robustness of RUL estimation methods for Li-ion batter-\\nies. However, deep learning models can be computationally\\nexpensive to train and require large amounts of data, which\\nmay limit their applicability in some domains.\\nIn recent years, the landscape of RUL estimation for Li-ion\\nbatteries has undergone a remarkable transformation with the\\nemergence of transformer-based models [5]. This paradigm\\nshift can be attributed to the extraordinary capabilities of\\ntransformers in processing sequential data efficiently, render-\\ning them exceptionally well-suited for intricate time-series\\nforecasting tasks such as RUL estimation. Transformers have\\nrisen to prominence due to their innate prowess in capturing\\nlong-term dependencies within data, endowing them with\\nthe capacity to model the intricate and evolving degradation\\npatterns of Li-ion batteries with precision.\\nAPPENDIX B\\nEVALUATION METRICS\\nA. Relative Error (RE):\\nRelative Error measures the relative difference between\\npredicted and actual values and is represented as follows:\\nRE=|Y−ˆY|\\n|Y|(19)\\nwhere, ˆYrepresents the predicted value, and Yrepresents\\nthe actual value.B. Root Mean Square Error (RMSE):\\nRMSE calculates the square root of the mean of the squared\\ndifferences between predicted and actual values:\\nRMSE =vuut1\\nnnX\\ni=1(ˆYi−Yi)2 (20)\\nwhere, ˆYirepresents the predicted value for the i-th sample,\\nYirepresents the actual value for the i-th sample, and nis the\\ntotal number of samples.\\nC. Mean Absolute Error (MAE):\\nMAE calculates the mean of the absolute differences be-\\ntween predicted and actual values:\\nMAE =1\\nnnX\\ni=1|ˆYi−Yi| (21)\\nwhere, ˆYirepresents the predicted value for the i-th sample,\\nYirepresents the actual value for the i-th sample, and nis the\\ntotal number of samples.\\nAPPENDIX C\\nHIGH-LEVEL OVERVIEW OF THE EXPERIMENTAL\\nFRAMEWORK\\nThe initial phase of the experimental framework is shown\\nin Fig. 6 in which the input data is standardized to establish\\na uniform scale, ensuring coherence. Following that, the\\ndenoising modules are used, which are designed to refine\\nthe data by filtering out extraneous noise, thereby improving\\nthe dataset’s integrity. The transformer-encoder component\\nextracts intricate features from refined data and computes\\ncritical metrics like RE, MAE, and RMSE. This sequential\\nprotocol, encapsulates the series of operations that form the\\nfoundation of our experimental pipeline.\\nInputs\\nNormalization\\nGaussian\\nNoiseSubjected to\\nDenoising ModulesSpeckle\\nNoisePoisson\\nNoiseUniform\\nNoiseWavelet\\nDecomposition\\nEncoder Encoder Encoder Encoder Encoder\\nMinimum Layer\\nMetric (RE, MAE,\\nRMSE)Attention Layers\\nFig. 6: High-level overview of the experimental frameworkAPPENDIX D\\nABBREVIATIONS\\nBelow we list all the abbreviations used in this paper.\\nLi-ion Lithium-ion\\nRUL Remaining Useful Life\\nPHM Prognostics and Health Management\\nEOL End of Life\\nRVM Relevance Vector Machine\\nSOH State of Health\\nNASA National Aeronautics and Space Administration\\nCALCE Center for Advanced Life Cycle Engineering\\nLSTM Long Short Term Memory\\nANN Artificial Neural Network\\nTCN Temporal Convolution Network\\nCNN Convolutional Neural Network\\nRNN Recurrent Neural network\\nPDF Probability Density Function\\nRE Relative Error\\nRMSE Root Mean Squared Error\\nMAE Mean Absolute Error\\nDWT Discrete Wavelet Transform\\nIDWT Inverse Discrete Wavelet Transform\\nNoL Number of layers\\nHD Hidden Dimensions\\nLR Learning rate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator= '\\n',\n",
        "    chunk_size= 1300 ,\n",
        "    chunk_overlap=200, #import for context\n",
        "    length_function=len\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "mejG2CixdFEq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_text(raw_content)"
      ],
      "metadata": {
        "id": "1MW7MCh_eGxA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyKTk0dHeREy",
        "outputId": "5a0a64d0-48b4-4c8b-aa5f-26af2fc71ddc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "J6Whsseled_x"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search = FAISS.from_texts(texts,embeddings)"
      ],
      "metadata": {
        "id": "wJ4-Y6z_ehQi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3KFrnKWevCr",
        "outputId": "f7338f7e-087e-4dfc-8a0e-413958d110ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x78297b8cda20>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "gbpJD9KZew0R"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(),chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "LG3rWa96e_By"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"What is the key standpout point in the De-SaTE paper\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents = docs, question = query )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1zvgZe58fKo6",
        "outputId": "a1176320-557a-4000-d846-f3ccd6083d10"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The key standout point in the De-SaTE paper is the use of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data, to enhance data representation and accuracy in predicting Remaining Useful Life (RUL) of Lithium-ion (Li-ion) batteries.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}